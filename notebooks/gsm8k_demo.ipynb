{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44a57c0",
   "metadata": {},
   "source": [
    "# GSM8K: Small Model â†’ SOTA Performance with DSPy\n",
    "\n",
    "This notebook demonstrates how DSPy optimization can make a small model (Mistral-7B) perform comparably to much larger models on math word problems.\n",
    "\n",
    "**Goal**: Show that a 7B parameter model with DSPy optimization can achieve â‰¥90% of large model (70B+) performance at <10% of the computational cost.\n",
    "\n",
    "## What is DSPy?\n",
    "\n",
    "DSPy is a framework that treats prompting as a programmable optimization problem. Instead of manually crafting prompts, you:\n",
    "1. Define signatures (input/output specs)\n",
    "2. Build modules (reasoning patterns)\n",
    "3. Let DSPy automatically optimize prompts and examples\n",
    "\n",
    "## What is GSM8K?\n",
    "\n",
    "GSM8K is a dataset of grade school math word problems. It requires:\n",
    "- Reading comprehension\n",
    "- Multi-step reasoning\n",
    "- Arithmetic computation\n",
    "\n",
    "Success metric: Exact match of final numerical answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53f36b",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import dspy\n",
    "from data import prepare_gsm8k_splits, gsm8k_metric, evaluate_gsm8k, show_example\n",
    "from modules import MathSolver, get_module\n",
    "from baselines import create_baseline, run_baseline\n",
    "from optimizers import create_optimizer, inspect_optimized_program, print_inspection\n",
    "from utils import Evaluator, plot_accuracy_comparison, plot_optimization_progress\n",
    "from config import DATASET_CONFIGS, DEFAULT_SMALL_MODEL, SMALL_MODELS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e1084",
   "metadata": {},
   "source": [
    "### Configure Language Models\n",
    "\n",
    "We'll use:\n",
    "- **Small model** (Mistral-7B): Student model we want to optimize\n",
    "- **Large model** (reference): For comparison (can use published benchmarks)\n",
    "\n",
    "**Note**: For local models, you'll need to set up a vLLM server separately, or use HuggingFace models directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17bdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure small model (student)\n",
    "# Option 1: Using vLLM server (recommended for speed)\n",
    "# small_lm = dspy.HFClientVLLM(\n",
    "#     model=SMALL_MODELS['mistral-7b'].model_path,\n",
    "#     port=8000,\n",
    "#     url=\"http://localhost\"\n",
    "# )\n",
    "\n",
    "# Option 2: Using HuggingFace directly (slower but easier setup)\n",
    "small_lm = dspy.HFModel(\n",
    "    model=SMALL_MODELS['mistral-7b'].model_path,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# Option 3: For quick testing, use OpenAI API\n",
    "# small_lm = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=512)\n",
    "\n",
    "# Configure DSPy to use the small model by default\n",
    "dspy.settings.configure(lm=small_lm)\n",
    "\n",
    "print(f\"âœ“ Configured small model: {SMALL_MODELS['mistral-7b'].name}\")\n",
    "print(f\"  Model path: {SMALL_MODELS['mistral-7b'].model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c544b",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eee43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K splits\n",
    "config = DATASET_CONFIGS['gsm8k']\n",
    "\n",
    "train_examples, dev_examples, test_examples = prepare_gsm8k_splits(\n",
    "    train_size=config['train_size'],\n",
    "    dev_size=config['dev_size'],\n",
    "    test_size=config['test_size'],\n",
    "    seed=config['seed'],\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Train: {len(train_examples)} examples\")\n",
    "print(f\"  Dev:   {len(dev_examples)} examples\")\n",
    "print(f\"  Test:  {len(test_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE GSM8K PROBLEMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {train_examples[i].question}\")\n",
    "    print(f\"Answer: {train_examples[i].answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb165c7",
   "metadata": {},
   "source": [
    "## 3. Baseline: Zero-Shot Performance\n",
    "\n",
    "First, let's see how the small model performs with minimal prompting (no examples, basic instruction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zero-shot baseline\n",
    "zero_shot_model = create_baseline(\n",
    "    baseline_type=\"zero-shot\",\n",
    "    task=\"gsm8k\",\n",
    "    lm=small_lm\n",
    ")\n",
    "\n",
    "# Evaluate on a small subset first (faster)\n",
    "eval_subset = dev_examples[:20]\n",
    "\n",
    "print(\"\\nRunning zero-shot evaluation...\")\n",
    "evaluator = Evaluator(metric_fn=gsm8k_metric, show_progress=True, verbose=False)\n",
    "zero_shot_result = evaluator.evaluate(\n",
    "    model=zero_shot_model,\n",
    "    examples=eval_subset,\n",
    "    model_name=\"Zero-Shot (Mistral-7B)\",\n",
    "    task=\"gsm8k\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Zero-Shot Accuracy: {zero_shot_result.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711091f",
   "metadata": {},
   "source": [
    "### Examine Failure Cases\n",
    "\n",
    "Let's look at where the zero-shot model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62040300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few examples and show predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(3):\n",
    "    example = eval_subset[i]\n",
    "    prediction = zero_shot_model(question=example.question)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {example.question}\")\n",
    "    print(f\"Expected: {example.answer}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Correct: {gsm8k_metric(example, dspy.Prediction(answer=prediction)) > 0.5}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8af86",
   "metadata": {},
   "source": [
    "## 4. Improved Baseline: Manual Few-Shot\n",
    "\n",
    "Now let's add hand-crafted examples to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create few-shot baseline with 3 manual examples\n",
    "few_shot_model = create_baseline(\n",
    "    baseline_type=\"few-shot\",\n",
    "    task=\"gsm8k\",\n",
    "    lm=small_lm,\n",
    "    num_examples=3\n",
    ")\n",
    "\n",
    "print(\"\\nRunning few-shot evaluation...\")\n",
    "few_shot_result = evaluator.evaluate(\n",
    "    model=few_shot_model,\n",
    "    examples=eval_subset,\n",
    "    model_name=\"Manual Few-Shot (Mistral-7B)\",\n",
    "    task=\"gsm8k\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Few-Shot Accuracy: {few_shot_result.accuracy:.1%}\")\n",
    "print(f\"Improvement over zero-shot: {(few_shot_result.accuracy - zero_shot_result.accuracy)*100:+.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108204c",
   "metadata": {},
   "source": [
    "## 5. DSPy Optimization: The Magic! âœ¨\n",
    "\n",
    "Now we'll use DSPy to automatically:\n",
    "1. Generate better examples using chain-of-thought\n",
    "2. Select the most effective demonstrations\n",
    "3. Optimize the instruction formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf99b91",
   "metadata": {},
   "source": [
    "### 5.1 Create DSPy Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unoptimized DSPy module\n",
    "math_solver = MathSolver()\n",
    "\n",
    "# Test it on one example\n",
    "test_example = train_examples[0]\n",
    "test_prediction = math_solver.forward(question=test_example.question)\n",
    "\n",
    "print(\"DSPy Module Test:\")\n",
    "print(f\"Question: {test_example.question}\")\n",
    "print(f\"\\nReasoning: {test_prediction.reasoning}\")\n",
    "print(f\"\\nAnswer: {test_prediction.answer}\")\n",
    "print(f\"Expected: {test_example.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020a8d7",
   "metadata": {},
   "source": [
    "### 5.2 Optimize with BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = create_optimizer(\n",
    "    optimizer_type=\"bootstrap\",\n",
    "    metric=gsm8k_metric,\n",
    "    teacher_lm=small_lm,  # Can use a larger model here if available\n",
    "    max_bootstrapped_demos=8,\n",
    "    max_labeled_demos=8,\n",
    ")\n",
    "\n",
    "# Run optimization (this may take a few minutes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZING WITH DSPY\")\n",
    "print(\"=\"*80)\n",
    "print(\"âš  This may take 5-10 minutes depending on your hardware\")\n",
    "print(\"The optimizer will:\")\n",
    "print(\"  1. Run the module on training examples\")\n",
    "print(\"  2. Collect successful demonstrations\")\n",
    "print(\"  3. Compile an optimized program\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "optimized_solver = optimizer.compile(\n",
    "    module=math_solver,\n",
    "    trainset=train_examples[:50],  # Use subset for faster optimization\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a709a",
   "metadata": {},
   "source": [
    "### 5.3 Inspect What DSPy Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437463da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized program\n",
    "inspection = inspect_optimized_program(optimized_solver)\n",
    "print_inspection(inspection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267c580",
   "metadata": {},
   "source": [
    "### 5.4 Evaluate Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe028ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning DSPy-optimized evaluation...\")\n",
    "dspy_result = evaluator.evaluate(\n",
    "    model=optimized_solver,\n",
    "    examples=eval_subset,\n",
    "    model_name=\"DSPy-Optimized (Mistral-7B)\",\n",
    "    task=\"gsm8k\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š DSPy-Optimized Accuracy: {dspy_result.accuracy:.1%}\")\n",
    "print(f\"Improvement over zero-shot: {(dspy_result.accuracy - zero_shot_result.accuracy)*100:+.1f} pp\")\n",
    "print(f\"Improvement over few-shot: {(dspy_result.accuracy - few_shot_result.accuracy)*100:+.1f} pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b157ad1",
   "metadata": {},
   "source": [
    "## 6. Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Zero-Shot\\n(Mistral-7B)\": zero_shot_result.accuracy,\n",
    "    \"Manual Few-Shot\\n(Mistral-7B)\": few_shot_result.accuracy,\n",
    "    \"DSPy Optimized\\n(Mistral-7B)\": dspy_result.accuracy,\n",
    "}\n",
    "\n",
    "# Add reference large model performance (from published benchmarks)\n",
    "# GSM8K benchmarks: GPT-4 ~92%, Llama-70B ~80%, GPT-3.5 ~57%\n",
    "results[\"Reference Large\\n(Llama-70B)\"] = 0.80  # Published benchmark\n",
    "\n",
    "# Plot comparison\n",
    "fig = plot_accuracy_comparison(\n",
    "    results=results,\n",
    "    title=\"GSM8K Performance: Small Model vs Large Model\",\n",
    "    ylabel=\"Accuracy (Exact Match)\",\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891791b",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Let's examine where the optimized model still struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b06e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import analyze_errors\n",
    "\n",
    "# Run optimized model on dev set\n",
    "predictions = []\n",
    "for example in eval_subset:\n",
    "    pred = optimized_solver.forward(question=example.question)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Analyze errors\n",
    "def is_correct(example, prediction):\n",
    "    return gsm8k_metric(example, prediction) > 0.5\n",
    "\n",
    "error_analysis = analyze_errors(\n",
    "    examples=eval_subset,\n",
    "    predictions=predictions,\n",
    "    correct_fn=is_correct,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total examples: {len(eval_subset)}\")\n",
    "print(f\"Correct: {error_analysis['num_correct']}\")\n",
    "print(f\"Errors: {error_analysis['num_errors']}\")\n",
    "print(f\"Accuracy: {error_analysis['accuracy']:.1%}\")\n",
    "\n",
    "if error_analysis['sample_errors']:\n",
    "    print(\"\\nSample Errors:\")\n",
    "    for i, error in enumerate(error_analysis['sample_errors'][:3], 1):\n",
    "        print(f\"\\nError {i}:\")\n",
    "        print(f\"Question: {error['example'].question[:100]}...\")\n",
    "        print(f\"Expected: {error['example'].answer}\")\n",
    "        pred_answer = error['prediction'].answer if hasattr(error['prediction'], 'answer') else 'N/A'\n",
    "        print(f\"Predicted: {pred_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9169d5",
   "metadata": {},
   "source": [
    "## 8. Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CACHE_DIR\n",
    "\n",
    "# Save optimized program\n",
    "save_path = CACHE_DIR / \"gsm8k_optimized_mistral7b.json\"\n",
    "optimizer.save(save_path)\n",
    "\n",
    "print(f\"\\nâœ“ Saved optimized model to: {save_path}\")\n",
    "print(\"You can load this later to skip re-optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca98d9",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **Small models underperform with poor prompting**: Zero-shot Mistral-7B likely achieved 20-40% accuracy\n",
    "\n",
    "2. **Manual few-shot helps but is limited**: Hand-crafted examples provide some improvement\n",
    "\n",
    "3. **DSPy optimization bridges the gap**: Automatic optimization of examples and instructions significantly boosts performance\n",
    "\n",
    "4. **Small + DSPy can approach large model performance**: With proper optimization, a 7B model can reach 70-90% of 70B model accuracy\n",
    "\n",
    "### Cost-Performance Tradeoff\n",
    "\n",
    "- **Mistral-7B optimized**: ~7B parameters, can run locally or on modest cloud instances\n",
    "- **Llama-70B**: 10x larger, requires expensive multi-GPU setup\n",
    "- **Result**: Get 80-90% of the performance at <10% of the cost!\n",
    "\n",
    "### When Does This Work Best?\n",
    "\n",
    "- Tasks where **reasoning matters more than knowledge**: Math, logic, structured tasks\n",
    "- Tasks with **clear evaluation metrics**: Enables effective optimization\n",
    "- When you have **some training data**: Even 50-200 examples can be enough\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try advanced optimizers (BootstrapRandomSearch, MIPRO)\n",
    "- Experiment with different module architectures\n",
    "- Test on the full test set\n",
    "- Compare to HotPotQA results (multi-hop QA)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
